{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mlmachine - GroupbyImputer, KFoldEncoder, and Skew Correction__\n",
    "<br><br>\n",
    "Welcome to Example Notebook 2. If you're new to mlmachine, check out [Example Notebook 1](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_1.ipynb).\n",
    "<br><br>\n",
    "Check out the [GitHub repository](https://github.com/petersontylerd/mlmachine).\n",
    "<br><br>\n",
    "\n",
    "1. [Missing Values - Assessment & GroupbyImputer](#Missing-Values-Assessment-&-GroupbyImputer)\n",
    "    1. [Assessment](#Assessment)\n",
    "    1. [GroupbyImputer](#GroupbyImputer)\n",
    "    1. [Imputation](#Imputation)\n",
    "1. [KFold Encoding - Exotic Encoding Without the Leakage](#KFold-Encoding-Exotic-Encoding-Without-the-Leakage)\n",
    "    1. [KFoldEncoder](#KFoldEncoder)\n",
    "1. [Box, Cox, Yeo & Johnson - Skew Correctors](#Box,-Cox,-Yeo-&-Johnson-Skew-Correctors)\n",
    "    1. [Assessment](#Assessment-1)\n",
    "    1. [Skew correction](#Skew-correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Missing Values - Assessment & GroupbyImputer\n",
    "---\n",
    "<br><br>\n",
    "Let's start by instantiating a couple `Machine()` objects, one for our training data and a second for our validation data\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Missing-Values-Assessment-&-GroupbyImputer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.116611Z",
     "start_time": "2020-04-03T02:07:10.050330Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import mlmachine tools\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "\n",
    "# use titanic() function to create DataFrames for training and validation datasets\n",
    "df_train, df_valid = titanic()\n",
    "\n",
    "# ordinal encoding hierarchy\n",
    "ordinal_encodings = {\"Pclass\": [1, 2, 3]}\n",
    "\n",
    "# instantiate a Machine object for the training data\n",
    "mlmachine_titanic_train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "# instantiate a Machine object for the validation data\n",
    "mlmachine_titanic_valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assessment\n",
    "---\n",
    "<br><br>\n",
    "Each  `Machine()` object  contains a method for summarizing missingness in tabular form and in graphical form:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Assessment'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.329636Z",
     "start_time": "2020-04-03T02:07:12.117608Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate missingness summary for training data\n",
    "mlmachine_titanic_train.eda_missing_summary(display_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "By default, this method acts on the `data` attribute associated with `mlmachine_train`. Let's do the same for the validation dataset:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.493657Z",
     "start_time": "2020-04-03T02:07:12.330636Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate missingness summary for validation data\n",
    "mlmachine_titanic_valid.eda_missing_summary(display_df=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Next, we need to determine if there are features with missing values in the training data, but not the validation data, and vice versa. This informs how we should set up our transformation pipeline. For example, if a feature has missing values in the validation dataset, but not the training dataset, we will still want to fit_transform this feature on the training data to learn imputation values to apply on the nulls in the validation dataset.\n",
    "<br><br>\n",
    "\n",
    "We could eyeball the tables and visuals above to compare the state of missingness in the two datasets, but this can be tedious, particularly with large datasets. Instead, we will leverage a method within our `Machine()` object. We simply pass the validation dataset to `mlmachine_titanic_train`'s method `missing_col_compare`, which returns a bidirectional missingness summary.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.502663Z",
     "start_time": "2020-04-03T02:07:12.494663Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate missingness comparison summary\n",
    "mlmachine_titanic_train.missing_column_compare(\n",
    "    validation_data=mlmachine_titanic_valid.data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The key observation here is that \"Fare\" is fully populated in the training data, but not the validation data. We need to make sure our pipeline learns how to impute these missing values based on the training data, despite the fact that the training data is not missing any values in this feature.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## GroupbyImputer\n",
    "---\n",
    "<br><br>\n",
    "mlmachine includes a transformer called `GroupbyImputer()`, which makes it easy to perform the same basic imputation techniques provided by Scikit-learn's `SimpleImputer()`, but with the added ability to group by another feature in the dataset. Let's see an example:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'GroupbyImputer'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.531665Z",
     "start_time": "2020-04-03T02:07:12.503660Z"
    }
   },
   "outputs": [],
   "source": [
    "# import mlmachine tools\n",
    "from mlmachine.features.preprocessing import GroupbyImputer\n",
    "\n",
    "# instantiate GroupbyImputer to fill \"Age\" mean, grouped by \"SibSp\"\n",
    "impute = GroupbyImputer(null_column=\"Age\", groupby_column=\"SibSp\", strategy=\"mean\")\n",
    "impute.fit_transform(mlmachine_titanic_train.data[[\"Age\",\"SibSp\"]])\n",
    "display(impute.train_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "In the code snippet above, we mean impute \"Age\", grouped by \"SibSp\". We pass \"Age\" to the `null_column` parameter to indicate which column contains the nulls, and pass \"SibSp\" to the `groupby_column` parameter. The strategy parameter receives the same instructions as Scikit-learn's `SimpleImputer()` - \"mean\", \"median\" and \"most_frequent\".\n",
    "<br><br>\n",
    "\n",
    "To inspect the learned values, we can display the object's `train_value` attribute, which is a `DataFrame` containing the category/value pairs\n",
    "<br><br>\n",
    "\n",
    "`GroupbyImputer` uses these pairs to impute the missing values in \"Age\". If, in the unlikely circumstance, a level in `groupby_column` has only null values in `null_column`, then the missing values associated with that level will be imputed with the mean, median or mode of the entire feature.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Imputation\n",
    "---\n",
    "<br><br>\n",
    "Now we're going to use `GroupbyImputer()` within `PandasFeatureUnion()` to impute nulls in both the training and validation datasets.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Imputation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.577668Z",
     "start_time": "2020-04-03T02:07:12.532664Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# import mlmachine tools\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    PandasFeatureUnion,\n",
    ")\n",
    "\n",
    "# create imputation PandasFeatureUnion pipeline\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"age\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\",\"SibSp\"]),\n",
    "        GroupbyImputer(null_column=\"Age\", groupby_column=\"SibSp\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Fare\",\"Pclass\"]),\n",
    "        GroupbyImputer(null_column=\"Fare\", groupby_column=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"embarked\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"cabin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Cabin\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=\"X\"))\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"Age\",\"Fare\",\"Embarked\",\"Cabin\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = impute_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = impute_pipe.transform(mlmachine_titanic_valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.593671Z",
     "start_time": "2020-04-03T02:07:12.578668Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic_train.data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "`GroupbyImputer()` makes two appearances in this `PandasFeatureUnion()` operation. On line 4, we groupby the feature \"SibSp\" to impute the mean \"Age\" value, and on line 8 we groupby the feature \"Pclass\" to impute the mean \"Fare\" value. \n",
    "<br><br>\n",
    "\n",
    "Imputations for \"Embarked\" and \"Cabin\" are completed in straightforward fashion - \"Embarked\" is simply imputed with the mode, and \"Cabin\" is imputed with the constant value of \"X\".\n",
    "<br><br>\n",
    "\n",
    "Lastly, we `fit_transform()` the `PandasFeatureUnion()` on `mlmachine_titanic_train.data` and finish filling our nulls by calling `transform()` on `mlmachine_titanic_valid.data`.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# KFold Encoding - Exotic Encoding Without the Leakage\n",
    "---\n",
    "<br><br>\n",
    "Target value-based encoding techniques such as mean encoding, CatBoost Encoding, and Weight of Evidence encoding are often discussed in the context of Kaggle competitions. The primary advantage of these techniques is that they use the target variable to inform the encoded feature's values. However, this comes with the risk of leaking target information into the encoded values. \n",
    "<br><br>\n",
    "\n",
    "KFold cross-validation assists in avoiding this problem. The key is to apply the encoded values to the out-of-fold observations only. This visualization illustrates the general pattern:\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p3_kfold.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "- Separate a validation subset from the training dataset.\n",
    "- Learn the encoded values from the training data and the associated target values.\n",
    "- Apply the learned values to the validation observations only.\n",
    "- Repeat the process on the K-1 remaining folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'KFold-Encoding-Exotic-Encoding-Without-the-Leakage'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## KFoldEncoder\n",
    "---\n",
    "<br><br>\n",
    "mlmachine has a class called `KFoldEncoder` that facilitates KFold encoding with an encoder of choice. Let's use a small subset of our features to see how this works. \n",
    "<br><br>\n",
    "\n",
    "We want to target encode two features: \"Pclass\" and \"Age\". Since \"Age\" is a continuous feature, we first need to map the values to bins, which is effectively an ordinal categorical column. We handle all of this in the simple PandasFeatureUnion below:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'KFoldEncoder'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.623672Z",
     "start_time": "2020-04-03T02:07:12.595668Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# create simple encoding PandasFeatureUnion pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\"]),\n",
    "        PandasTransformer(KBinsDiscretizer(encode=\"ordinal\"))\n",
    "    )),\n",
    "    (\"select\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\",\"Pclass\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = encode_pipe.fit_transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "This operation returns a binned version of \"Age\", as well as the original \"Age\" and \"Pclass\" features.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T02:07:12.634676Z",
     "start_time": "2020-04-03T02:07:12.624680Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Next, we target encode both \"Pclass\" and \"Age_binned_5\" using mean encoding, CatBoost encoding and Weight of Evidence encoding as provided by the package category_encoders. \n",
    "<br><br>\n",
    "\n",
    "We want to target encode two features: \"Pclass\" and \"Age\". Since \"Age\" is a continuous feature, we first need to map the values to bins, which is effectively an ordinal categorical column. We handle all of this in the simple PandasFeatureUnion below:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:03:27.150080Z",
     "start_time": "2020-03-30T16:03:26.202370Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.model_selection import KFold\n",
    "from category_encoders import WOEEncoder, TargetEncoder, CatBoostEncoder\n",
    "\n",
    "# import mlmachine tools\n",
    "from mlmachine.features.preprocessing import KFoldEncoder\n",
    "\n",
    "# create KFold encoding PandasFeatureUnion pipeline\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"], exclude_columns=[\"Cabin\"]),        \n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=0),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"woe\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=WOEEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = target_encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = target_encode_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:03:27.170384Z",
     "start_time": "2020-03-30T16:03:27.152065Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Let's review the key `KFoldEncoder()` parameters:\n",
    "- `target`: the target attribute of our mlmachine_titanic_train object\n",
    "- `cv`: a cross-validation object\n",
    "- `encoder`: a target encoder class\n",
    "<br><br>\n",
    "\n",
    "`KFoldEncoder()` learns the encoded values on the training data, and applies the values to the out-of-fold observations. \n",
    "<br><br>\n",
    "\n",
    "On the validation data, the process is simpler: we calculate the average out-of-fold encodings applied to the training data and apply these values to all validation observations.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Box, Cox, Yeo & Johnson - Skew Correctors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Box,-Cox,-Yeo-&-Johnson-Skew-Correctors'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Assessment\n",
    "---\n",
    "<br><br>\n",
    "Just as we have a quick method for evaluating missingness, we have a quick method for evaluating skew.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Assessment-1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:03:27.216492Z",
     "start_time": "2020-03-30T16:03:27.173993Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate skewness summary\n",
    "mlmachine_titanic_train.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The `skew_summary()` method returns a `DataFrame` that summarizes the skew for each feature, along with a \"Percent zero\" column, which informs us of the percentage of values in the feature that are zero.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Skew correction\n",
    "---\n",
    "<br><br>\n",
    "mlmachine contains a class called `DualTransformer()`, which, by default, applies both Yeo-Johnson and Box-Cox transformations to the specified features with the intent of correcting skew. The Box-Cox transformation automatically seeks the lambda value which maximizes the log-likelihood function. \n",
    "<br><br>\n",
    "\n",
    "Since Box-Cox transformation requires all values in a feature to be greater than zero, `DualTransformer()` applies one of two simple feature adjustments when this rule is violated:\n",
    "<br><br>\n",
    "- If the minimum value in a feature is zero, each value in that feature is increased by a value of 1 prior to transformation. \n",
    "- If the minimum value is less than zero, then each feature value is increased by the absolute value of the minimum value in the feature plus 1 prior to transformation.\n",
    "<br><br>\n",
    "\n",
    "Let's use `DualTransformer()` to see if we can minimize the skew in the original \"Age\" feature:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Skew-correction'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:03:27.260835Z",
     "start_time": "2020-03-30T16:03:27.220219Z"
    }
   },
   "outputs": [],
   "source": [
    "# import mlmachine tools\n",
    "from mlmachine.features.preprocessing import DualTransformer\n",
    "\n",
    "# create skew correction PandasFeatureUnion pipeline\n",
    "skew_pipe = PandasFeatureUnion([\n",
    "    (\"skew\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\"]),\n",
    "        DualTransformer(),\n",
    "    )),    \n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = skew_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = skew_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:03:27.276515Z",
     "start_time": "2020-03-30T16:03:27.264256Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "`DualTransformer()` adds the features \"Age_BoxCox\" and \"Age_YeoJohnson\". Let's execute `skew_summary()` again to see if `DualTransformer()` addressed the skew in our original feature:\n",
    "<br><br>\n",
    "\"Age_BoxCox\" and \"Age_YeoJohnson\" have a skew of 0.0286 and 0.0483, respectively.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:03:27.305056Z",
     "start_time": "2020-03-30T16:03:27.279561Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate skewness summary\n",
    "mlmachine_titanic_train.skew_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Star the [GitHub repository](https://github.com/petersontylerd/mlmachine), and stay tuned for additional notebooks.\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
