{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mlmachine - Part 4: Feature Selection__\n",
    "\n",
    "1. [Crowd-Sourced Feature Importance Estimation](#Crowd-Sourced-Feature-Importance-Estimation)\n",
    "    1. [Catalog of Techniques](#Catalog-of-Techniques)\n",
    "    1. [Prepare Data](#Prepare-Data)\n",
    "    1. [FeatureSelector](#FeatureSelector)\n",
    "        1. [Example 1 - Estimator Classes](#Example-1-Estimator-Classes)\n",
    "        1. [Example 2 - Instantiated Models](#Example-2-Instantiated-Models)\n",
    "    1. [Crowd-sourcing](#Crowd-sourcing)\n",
    "1. [Feature Selection Through Iterative Cross-validation](#Feature-Selection-Through-Iterative-Cross-validation)\n",
    "    1. [CV Summary](#CV-Summary)\n",
    "    1. [Visualize Performance Curves](#Visualize-Performance-Curves)\n",
    "    1. [Results Summaries](#Results-Summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:44:38.204347Z",
     "start_time": "2020-03-18T03:44:36.438888Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, KBinsDiscretizer, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from category_encoders import WOEEncoder, TargetEncoder, CatBoostEncoder\n",
    "from xgboost import XGBClassifier\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    PandasFeatureUnion,\n",
    "    GroupbyImputer,\n",
    "    KFoldEncoder,\n",
    "    DualTransformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Crowd-Sourced Feature Importance Estimation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Crowd-Sourced-Feature-Importance-Estimation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Catalog of Techniques\n",
    "---\n",
    "<br><br>\n",
    "Here is a non-exhaustive list of feature importance estimation techniques:\n",
    "- Tree-based Feature Importance\n",
    "- Recursive Feature Elimination\n",
    "- Sequential Forward Selection\n",
    "- Sequential Backward Selection\n",
    "- F-value / p-value\n",
    "- Variance \n",
    "- Target Correlation\n",
    "<br><br>\n",
    "\n",
    "This battery of techniques stems from several different libraries. Ideally, we use all of these techniques (where applicable) to get a broad understanding of the role each feature plays in a machine learning problem. This is a cumbersome series of tasks.\n",
    "<br><br>\n",
    "\n",
    "Even if we took time to execute each method, disparate execution leads to disparate variables, making a holistic assessment tedious to compile.\n",
    "<br><br>\n",
    "\n",
    "mlmachine's `FeatureSelector` class makes it easy to run all of the feature importance estimation techniques listed above. Further, we can do this for a variety of estimators simultaneously. Let's see mlmachine in action. \n",
    "<br><br>\n",
    "\n",
    "First, we apply data preprocessing techniques to clean up our data. For those who have read [Part 1](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_1.ipynb), [Part 2](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_2.ipynb), and [Part 3](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_3.ipynb) of this article series, nothing in this block of code below is new:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Catalog-of-Techniques'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Data\n",
    "---\n",
    "<br><br>\n",
    "First, we apply data preprocessing techniques to clean up our data. For those who have read Part 1, Part 2, and Part 3 of this article series, nothing in this block of code below is new:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:44:40.482675Z",
     "start_time": "2020-03-18T03:44:38.206339Z"
    }
   },
   "outputs": [],
   "source": [
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "\n",
    "df_train, df_valid = titanic()\n",
    "\n",
    "ordinal_encodings = {\"Pclass\": [1, 2, 3]}\n",
    "\n",
    "mlmachine_titanic_train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "mlmachine_titanic_valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "### impute pipeline\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"age\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\",\"SibSp\"]),\n",
    "        GroupbyImputer(null_column=\"Age\", groupby_column=\"SibSp\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Fare\",\"Pclass\"]),\n",
    "        GroupbyImputer(null_column=\"Fare\", groupby_column=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"embarked\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"Age\",\"Fare\",\"Embarked\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "mlmachine_titanic_train.data = impute_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = impute_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "### encode & bin pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=mlmachine_titanic_train.data.mlm_dtypes[\"nominal\"]),\n",
    "        PandasTransformer(OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=mlmachine_titanic_train.data.mlm_dtypes[\"continuous\"]),\n",
    "        PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=mlmachine_titanic_train.data.mlm_dtypes[\"nominal\"] + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "mlmachine_titanic_train.data = encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = encode_pipe.fit_transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### impute pipeline\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]), \n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=0),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"woe\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=WOEEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "mlmachine_titanic_train.data = target_encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = target_encode_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### scale values\n",
    "scale = PandasTransformer(RobustScaler())\n",
    "\n",
    "mlmachine_titanic_train.data = scale.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = scale.transform(mlmachine_titanic_valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmachine_titanic_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### FeatureSelector\n",
    "---\n",
    "<br><br>\n",
    "Our `DataFrame` has been imputed, encoded in a variety of ways, and has several new features. We're ready for `FeatureSelector`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'FeatureSelector'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 1 - Estimator Classes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Example-1-Estimator-Classes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:45:48.838844Z",
     "start_time": "2020-03-18T03:44:40.483675Z"
    }
   },
   "outputs": [],
   "source": [
    "estimators = [\n",
    "    LogisticRegression,\n",
    "    XGBClassifier,\n",
    "]\n",
    "\n",
    "fs = mlmachine_titanic_train.FeatureSelector(\n",
    "    data=mlmachine_titanic_train.data,\n",
    "    target=mlmachine_titanic_train.target,\n",
    "    estimators=estimators,\n",
    ")\n",
    "feature_selector_summary = fs.feature_selector_suite(\n",
    "    sequential_scoring=\"accuracy\",\n",
    "    sequential_n_folds=0,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "To instantiate a `FeatureSelector` object, we pass the training data, labels and a list of estimators. This list of estimators can contain estimator class names, variables of instantiated models, or a combination of the two. In this example, we pass the class names for `LogisticRegression` and `XGBClassifier`, which leverages the default settings of the estimators.\n",
    "<br><br>\n",
    "\n",
    "Our `FeatureSelector` object contains built-in methods for each feature importance technique described above, along with a method called `feature_selector_suite`, which combines all techniques into a single execution. \n",
    "<br><br>\n",
    "\n",
    "To execute `feature_selector_suite`, we pass \"accuracy\" to the parameter `sequential_scoring` and 0 to the parameter `sequential_n_folds`. These parameters influence the sequential backward/forward algorithms. We also set `save_to_csv` to True to save the resulting `DataFrame` to a CSV. Here is our result:\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:45:48.863840Z",
     "start_time": "2020-03-18T03:45:48.839844Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "In the `DataFrame`, some of the columns capture the value of the metric, including F-value, P-value, variance, target correlation and Tree-based Feature Importance. The remaining columns capture the order in which a feature was selected or eliminated, depending on the underlying algorithm.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 2 - Instantiated Models\n",
    "---\n",
    "<br><br>\n",
    "Here is another example. This time, we instantiate `FeatureSelector` using a list of estimators containing the `RandomForestClassifier` class and three instantiated models. Each `RandomForestClassifier` is instantiated with a different value for the `max_depth` hyperparameter. Lastly, we pass \"roc_auc\" to `sequential_scoring` instead of \"accuracy\" as our scoring metric.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Example-2-Instantiated-Models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:54:49.383409Z",
     "start_time": "2020-03-18T03:45:59.486455Z"
    }
   },
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(max_depth=2)\n",
    "rf4 = RandomForestClassifier(max_depth=4)\n",
    "rf6 = RandomForestClassifier(max_depth=6)\n",
    "\n",
    "estimators = [\n",
    "    RandomForestClassifier,\n",
    "    rf2,\n",
    "    rf4,\n",
    "    rf6,\n",
    "]\n",
    "\n",
    "fs = mlmachine_titanic_train.FeatureSelector(\n",
    "    data=mlmachine_titanic_train.data,\n",
    "    target=mlmachine_titanic_train.target,\n",
    "    estimators=estimators,\n",
    ")\n",
    "feature_selector_summary = fs.feature_selector_suite(\n",
    "    sequential_scoring=\"roc_auc\",\n",
    "    sequential_n_folds=0,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:54:49.415416Z",
     "start_time": "2020-03-18T03:54:49.384409Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Notice that in this `DataFrame` we have columns for Recursive Feature Elimination and Tree-based Feature Importance named specifically for the instantiated models in our estimators list.\n",
    "<br><br>\n",
    "\n",
    "We've already compiled a tidy summary describing feature importance, but we're just getting started.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Crowd-sourcing\n",
    "---\n",
    "<br><br>\n",
    "The value of having all of this information in one place is obvious, but to enable a crowd-sourced, ensemble-like assessment of feature importance we first need to normalize these values. This facilitates ranking. \n",
    "<br><br>\n",
    "\n",
    "In addition to bringing the values on to the same scale, we need to ensure that we consistently handle the directionality of the values. For example, high F-values suggest important features, whereas with p-values its the low values that suggest important features.\n",
    "<br><br>\n",
    "\n",
    "mlmachine makes this easy to do. `FeatureSelector` contains a method called \n",
    "`feature_selector_stats` which applies this ranking, column by column. As an added bonus, `feature_selector_stats` adds several summary statistic columns describing each feature's ranks, and automatically sorts the features by best rank.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Crowd-sourcing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:54:49.443421Z",
     "start_time": "2020-03-18T03:54:49.416418Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_selector_summary = fs.feature_selector_stats(feature_selector_summary)\n",
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Notice 3 key differences in this updated `DataFrame`:\n",
    "- The values in each column in the original `DataFrame` now appear as ranks, where lower values indicate features of higher importance.\n",
    "- Additional summary statistic columns inserted on the left side of the `DataFrame`. \n",
    "- The `DataFrame` is sorted based on average rank, ascending .\n",
    "<br><br>\n",
    "\n",
    "According to this summary, the top three most important features in these models are \"Fare\", the mean-encoded version of \"Sex\", and \"Age\".\n",
    "<br><br>\n",
    "\n",
    "In this example, we executed `feature_selector_stats` as a second step, but we can skip this by setting the `add_stats` parameter in `feature_selector_suite` to True.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Selection Through Iterative Cross-validation\n",
    "---\n",
    "<br><br>\n",
    "Feature importance alone is a good start, but our ultimate goal to select only the most meaningful features for our forthcoming model training phase. \n",
    "<br><br>\n",
    "\n",
    "Our ranked and sorted feature importance summary is the cornerstone of this next step. To evaluate each subset, `FeatureSelector` uses an approach similar to Recursive Feature Elimination:\n",
    "- Train model with all features using cross-validation\n",
    "- Capture average performance on training and validation datasets\n",
    "- Remove least important (1 x step size) features from remaining available features \n",
    "- Repeat until feature list is depleted\n",
    "<br><br>\n",
    "\n",
    "To determine 'least important', we rely on the ranked and sorted feature importance summary. Each time we remove features, we remove the lowest ranked features from those remaining. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-Selection-Through-Iterative-Cross-validation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CV Summary\n",
    "---\n",
    "<br><br>\n",
    "Let's see mlmachine in action:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'CV-Summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:59:06.302121Z",
     "start_time": "2020-03-18T03:54:49.444430Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_summary = fs.feature_selector_cross_val(\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "    estimators=estimators,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=5,\n",
    "    step=1,\n",
    "    n_jobs=4,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our `FeatureSelector` object contains a method called `feature_selector_cross_val`. This executes the iterative feature subset evaluation and stores the result. Let's review the parameters:\n",
    "- `feature_selector_summary`: our ranked and sorted feature importance summary\n",
    "- `estimators`: the list of estimators we want to use to evaluate the features\n",
    "- `scoring`: one or more scoring metrics\n",
    "- `n_folds`: number of folds in cross-validation procedure\n",
    "- `step`: number of features to remove after each iteration\n",
    "- `save_to_csv`: specifies whether to save the results in a CSV\n",
    "<br><br>\n",
    "\n",
    "Let's review our results:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_summary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "On each row of the `DataFrame`, we see the performance of a single estimator on a certain feature subset based on a specific scoring metric. The column \"features dropped\" describes how many features have been removed from the full feature set. Remember, after each iteration, `feature_selector_cross_val` removes (1 x step) of the least important features, where importance is based on how `feature_selection_summary` is ranked and sorted.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualize Performance Curves\n",
    "---\n",
    "<br><br>\n",
    "`FeatureSelector` also has a built-in method for visualizing the training and validation scores for each subset:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Visualize-Performance-Curves'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:59:07.048239Z",
     "start_time": "2020-03-18T03:59:06.303118Z"
    }
   },
   "outputs": [],
   "source": [
    "fs.feature_selector_results_plot(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "    title_scale=0.8,\n",
    "    marker_on=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Here we see the training and validation accuracy scores trended for each of our 4 `RandomForestClassifier` models. Each chart header clearly informs us of the best validation accuracy score, as well as how many features were removed from the full feature set when achieving that score.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summaries\n",
    "---\n",
    "<br><br>\n",
    "Lastly, there are a couple utilities within FeatureSelector that help us summarize and utilize the results of our cross validation procedure. \n",
    "<br><br>\n",
    "\n",
    "First, the method `create_cross_val_features_df` presents a summary of the features used by each model when achieving its best validation score:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Results-Summaries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:59:07.070242Z",
     "start_time": "2020-03-18T03:59:07.049240Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_features_df = fs.create_cross_val_features_df(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our features form the index of this `DataFrame`, and our estimators are represented as columns. At the intersection of each value, an X indicates if that feature was used in the subset that achieved the best validation score. The \"count\" column totals the number of estimators that used a feature, and the `DataFrame` is ranked descending on this column. \n",
    "<br><br>\n",
    "\n",
    "Second, the method `create_cross_val_features_df` compiles the best feature subsets for each model and returns the results in a dictionary. The estimators are the keys and the associated values are list containing the best feature subset for each estimator.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:59:07.093250Z",
     "start_time": "2020-03-18T03:59:07.071242Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_feature_dict = fs.create_cross_val_features_dict(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "This dictionary facilitates quick utilization of these feature subsets in the model training phase.\n",
    "<br><br>\n",
    "Continue to [Part 5](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_5.ipynb)\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
