{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mlmachine - Hyperparameter Tuning with Bayesian Optimization__\n",
    "<br><br>\n",
    "Welcome to Example Notebook 4. If you're new to mlmachine, check out [Example Notebook 1](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_1.ipynb), [Example Notebook 2](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_2.ipynb) and [Example Notebook 3](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_3.ipynb).\n",
    "<br><br>\n",
    "Check out the [GitHub repository](https://github.com/petersontylerd/mlmachine).\n",
    "<br><br>\n",
    "\n",
    "1. [Bayesian Optimization for Multiple Estimators in One Shot](#Bayesian-Optimization-for-Multiple-Estimators-in-One-Shot)\n",
    "    1. [Prepare Data](#Prepare-Data)\n",
    "    1. [Feature Importance Summary](#Feature-Importance-Summary)\n",
    "    1. [Exhaustively Iterative Feature Selection](#Exhaustively-Iterative-Feature-Selection)\n",
    "    1. [Outline Our Feature Space](#Outline-Our-Feature-Space)\n",
    "    1. [Run the Bayesian Optimization Job](#Run-the-Bayesian-Optimization-Job)\n",
    "1. [Results Analysis](#Results-Analysis)\n",
    "    1. [Results Summary](#Results-Summary)\n",
    "    1. [Model Optimization Assessment](#Model-Optimization-Assessment)\n",
    "    1. [Parameter Selection Assessment](#Parameter-Selection-Assessment)\n",
    "1. [Model Reinstantiation](#Model-Reinstantiation)\n",
    "    1. [Top Model Identification](#Top-Model-Identification)\n",
    "    1. [Putting the Models to Use](#Putting-the-Models-to-Use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Bayesian Optimization for Multiple Estimators in One Shot\n",
    "---\n",
    "<br><br>\n",
    "Bayesian optimization is typically described as an advancement beyond exhaustive grid searches, and rightfully so. This hyperparameter tuning strategy succeeds by using prior information to inform future parameter selection for a given estimator. Check out Will Koerhsen's article on Medium for an excellent overview of the package.\n",
    "<br><br>\n",
    "\n",
    "mlmachine uses hyperopt as a foundation for performing Bayesian optimization, and takes the functionality of hyperopt a step further through a simplified workflow that allows for optimization of multiple models in single process execution. In this article, we are going to optimize four classifiers:\n",
    "- `LogisticRegression()`\n",
    "- `XGBClassifier()`\n",
    "- `RandomForestClassifier()`\n",
    "- `KNeighborsClassifier()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Bayesian-Optimization-for-Multiple-Estimators-in-One-Shot'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Data\n",
    "---\n",
    "<br><br>\n",
    "First, we apply data preprocessing techniques to clean up our data. We'll start by creating two `Machine()` objects - one for the training data and a second for the validation data.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:00:08.749890Z",
     "start_time": "2020-03-30T16:00:07.328537Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\petersont\\appdata\\local\\continuum\\anaconda3\\envs\\petersont\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> category label encoding\n",
      "\n",
      "\t0 --> 0\n",
      "\t1 --> 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import mlmachine tools\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "\n",
    "# use titanic() function to create DataFrames for training and validation datasets\n",
    "df_train, df_valid = titanic()\n",
    "\n",
    "# ordinal encoding hierarchy\n",
    "ordinal_encodings = {\"Pclass\": [1, 2, 3]}\n",
    "\n",
    "# instantiate a Machine object for the training data\n",
    "mlmachine_titanic_train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "# instantiate a Machine object for the validation data\n",
    "mlmachine_titanic_valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Now we process the data by imputing nulls and applying various binning, feature engineering and encoding techniques:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:00:12.636227Z",
     "start_time": "2020-03-30T16:00:08.753697Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import (\n",
    "    OrdinalEncoder,\n",
    "    OneHotEncoder,\n",
    "    KBinsDiscretizer,\n",
    "    RobustScaler,\n",
    "    PolynomialFeatures,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from category_encoders import WOEEncoder, TargetEncoder, CatBoostEncoder\n",
    "\n",
    "# import mlmachine tools\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    PandasFeatureUnion,\n",
    "    GroupbyImputer,\n",
    "    KFoldEncoder,\n",
    ")\n",
    "\n",
    "### create imputation PandasFeatureUnion pipeline\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"age\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\",\"SibSp\"]),\n",
    "        GroupbyImputer(null_column=\"Age\", groupby_column=\"SibSp\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Fare\",\"Pclass\"]),\n",
    "        GroupbyImputer(null_column=\"Fare\", groupby_column=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"embarked\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"Age\",\"Fare\",\"Embarked\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = impute_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = impute_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "### create polynomial feature PandasFeatureUnion pipeline\n",
    "polynomial_pipe = PandasFeatureUnion([\n",
    "    (\"polynomial\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        PandasTransformer(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"continuous\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = polynomial_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = polynomial_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### create simple encoding & binning PandasFeatureUnion pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=mlmachine_titanic_train.data.mlm_dtypes[\"nominal\"]),\n",
    "        PandasTransformer(OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=mlmachine_titanic_train.data.mlm_dtypes[\"continuous\"]),\n",
    "        PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=mlmachine_titanic_train.data.mlm_dtypes[\"nominal\"] + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = encode_pipe.fit_transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### create KFold encoding PandasFeatureUnion pipeline\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]), \n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=0),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"woe\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=WOEEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = target_encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = target_encode_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### scale values\n",
    "scale = PandasTransformer(RobustScaler())\n",
    "\n",
    "mlmachine_titanic_train.data = scale.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = scale.transform(mlmachine_titanic_valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:00:12.670423Z",
     "start_time": "2020-03-30T16:00:12.638971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age*Fare</th>\n",
       "      <th>Age*Fare_binned_5</th>\n",
       "      <th>Age*Fare_binned_5_catboost_encoded</th>\n",
       "      <th>Age*Fare_binned_5_target_encoded</th>\n",
       "      <th>Age*Fare_binned_5_woe_encoded</th>\n",
       "      <th>Age^2</th>\n",
       "      <th>Age^2_binned_5</th>\n",
       "      <th>Age^2_binned_5_catboost_encoded</th>\n",
       "      <th>Age^2_binned_5_target_encoded</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass_ordinal_encoded</th>\n",
       "      <th>Pclass_ordinal_encoded_catboost_encoded</th>\n",
       "      <th>Pclass_ordinal_encoded_target_encoded</th>\n",
       "      <th>Pclass_ordinal_encoded_woe_encoded</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_male_catboost_encoded</th>\n",
       "      <th>Sex_male_target_encoded</th>\n",
       "      <th>Sex_male_woe_encoded</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622287</td>\n",
       "      <td>-0.241862</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.061922</td>\n",
       "      <td>0.257553</td>\n",
       "      <td>-0.090433</td>\n",
       "      <td>-0.568680</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>0.754887</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.608483</td>\n",
       "      <td>2.880011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.746016</td>\n",
       "      <td>1.330575</td>\n",
       "      <td>1.479878</td>\n",
       "      <td>0.726867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.660192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.760707</td>\n",
       "      <td>1.370847</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.989747</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.314594</td>\n",
       "      <td>-0.184856</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.034485</td>\n",
       "      <td>-0.868374</td>\n",
       "      <td>-1.118681</td>\n",
       "      <td>-0.309570</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>-0.053122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.121216</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.991811</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377713</td>\n",
       "      <td>1.838762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.746016</td>\n",
       "      <td>1.529026</td>\n",
       "      <td>1.479878</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.550145</td>\n",
       "      <td>-0.358207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.760707</td>\n",
       "      <td>1.518257</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>1.018386</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.377713</td>\n",
       "      <td>-0.092152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.528664</td>\n",
       "      <td>-0.424139</td>\n",
       "      <td>-0.541412</td>\n",
       "      <td>0.431320</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.550145</td>\n",
       "      <td>-0.164268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.100602</td>\n",
       "      <td>-0.111967</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-1.034485</td>\n",
       "      <td>-0.626603</td>\n",
       "      <td>-1.118681</td>\n",
       "      <td>0.108522</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.550145</td>\n",
       "      <td>-0.301361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.035279</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.839252</td>\n",
       "      <td>2.992442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.746016</td>\n",
       "      <td>1.529026</td>\n",
       "      <td>1.479878</td>\n",
       "      <td>2.713372</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.660192</td>\n",
       "      <td>0.442971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.760707</td>\n",
       "      <td>1.518257</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.160748</td>\n",
       "      <td>-0.385571</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.061922</td>\n",
       "      <td>0.257553</td>\n",
       "      <td>-0.090433</td>\n",
       "      <td>-1.216453</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.956874</td>\n",
       "      <td>1.754877</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.237671</td>\n",
       "      <td>-0.069069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.528664</td>\n",
       "      <td>-0.325717</td>\n",
       "      <td>-0.541412</td>\n",
       "      <td>-0.238045</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>-0.196302</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.035279</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.989747</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.237671</td>\n",
       "      <td>0.078365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.528664</td>\n",
       "      <td>-0.358860</td>\n",
       "      <td>-0.541412</td>\n",
       "      <td>-0.957344</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.956874</td>\n",
       "      <td>1.193962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.909780</td>\n",
       "      <td>0.823122</td>\n",
       "      <td>0.797100</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.935964</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Age*Fare  Age*Fare_binned_5  Age*Fare_binned_5_catboost_encoded  \\\n",
       "0 -0.622287 -0.241862               -1.0                           -0.061922   \n",
       "1  0.608483  2.880011                1.0                            1.746016   \n",
       "2 -0.314594 -0.184856               -0.5                           -1.034485   \n",
       "3  0.377713  1.838762                1.0                            1.746016   \n",
       "4  0.377713 -0.092152                0.0                           -0.528664   \n",
       "5  0.100602 -0.111967               -0.5                           -1.034485   \n",
       "6  1.839252  2.992442                1.0                            1.746016   \n",
       "7 -2.160748 -0.385571               -1.0                           -0.061922   \n",
       "8 -0.237671 -0.069069                0.0                           -0.528664   \n",
       "9 -1.237671  0.078365                0.0                           -0.528664   \n",
       "\n",
       "   Age*Fare_binned_5_target_encoded  Age*Fare_binned_5_woe_encoded     Age^2  \\\n",
       "0                          0.257553                      -0.090433 -0.568680   \n",
       "1                          1.330575                       1.479878  0.726867   \n",
       "2                         -0.868374                      -1.118681 -0.309570   \n",
       "3                          1.529026                       1.479878  0.431320   \n",
       "4                         -0.424139                      -0.541412  0.431320   \n",
       "5                         -0.626603                      -1.118681  0.108522   \n",
       "6                          1.529026                       1.479878  2.713372   \n",
       "7                          0.257553                      -0.090433 -1.216453   \n",
       "8                         -0.325717                      -0.541412 -0.238045   \n",
       "9                         -0.358860                      -0.541412 -0.957344   \n",
       "\n",
       "   Age^2_binned_5  Age^2_binned_5_catboost_encoded  \\\n",
       "0            -0.5                         0.196885   \n",
       "1             1.0                         0.660192   \n",
       "2            -0.5                         0.196885   \n",
       "3             0.5                        -0.550145   \n",
       "4             0.5                        -0.550145   \n",
       "5             0.5                        -0.550145   \n",
       "6             1.0                         0.660192   \n",
       "7            -1.0                         1.956874   \n",
       "8            -0.5                         0.196885   \n",
       "9            -1.0                         1.956874   \n",
       "\n",
       "   Age^2_binned_5_target_encoded  ...  Parch  Pclass_ordinal_encoded  \\\n",
       "0                       0.754887  ...    0.0                     0.0   \n",
       "1                       0.000000  ...    0.0                    -2.0   \n",
       "2                      -0.053122  ...    0.0                     0.0   \n",
       "3                      -0.358207  ...    0.0                    -2.0   \n",
       "4                      -0.164268  ...    0.0                     0.0   \n",
       "5                      -0.301361  ...    0.0                     0.0   \n",
       "6                       0.442971  ...    0.0                    -2.0   \n",
       "7                       1.754877  ...    1.0                     0.0   \n",
       "8                      -0.196302  ...    2.0                     0.0   \n",
       "9                       1.193962  ...    0.0                    -1.0   \n",
       "\n",
       "   Pclass_ordinal_encoded_catboost_encoded  \\\n",
       "0                                -0.089623   \n",
       "1                                 1.760707   \n",
       "2                                -0.089623   \n",
       "3                                 1.760707   \n",
       "4                                -0.089623   \n",
       "5                                -0.089623   \n",
       "6                                 1.760707   \n",
       "7                                -0.089623   \n",
       "8                                -0.089623   \n",
       "9                                 0.909780   \n",
       "\n",
       "   Pclass_ordinal_encoded_target_encoded  Pclass_ordinal_encoded_woe_encoded  \\\n",
       "0                               0.000000                           -0.167183   \n",
       "1                               1.370847                            1.550231   \n",
       "2                              -0.121216                           -0.167183   \n",
       "3                               1.518257                            1.550231   \n",
       "4                              -0.092714                           -0.167183   \n",
       "5                              -0.035279                           -0.167183   \n",
       "6                               1.518257                            1.550231   \n",
       "7                               0.000000                           -0.167183   \n",
       "8                              -0.035279                           -0.167183   \n",
       "9                               0.823122                            0.797100   \n",
       "\n",
       "   Sex_male  Sex_male_catboost_encoded  Sex_male_target_encoded  \\\n",
       "0       0.0                   0.017268                 0.017703   \n",
       "1      -1.0                   0.996585                 0.989747   \n",
       "2      -1.0                   0.996585                 0.991811   \n",
       "3      -1.0                   0.996585                 1.018386   \n",
       "4       0.0                   0.017268                -0.004054   \n",
       "5       0.0                   0.017268                 0.000000   \n",
       "6       0.0                   0.017268                 0.017703   \n",
       "7       0.0                   0.017268                 0.017703   \n",
       "8      -1.0                   0.996585                 0.989747   \n",
       "9      -1.0                   0.996585                 0.935964   \n",
       "\n",
       "   Sex_male_woe_encoded  SibSp  \n",
       "0              0.000604    1.0  \n",
       "1              0.991390    1.0  \n",
       "2              0.991390    0.0  \n",
       "3              0.991390    1.0  \n",
       "4              0.000604    0.0  \n",
       "5              0.000604    0.0  \n",
       "6              0.000604    0.0  \n",
       "7              0.000604    3.0  \n",
       "8              0.991390    0.0  \n",
       "9              0.991390    1.0  \n",
       "\n",
       "[10 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlmachine_titanic_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Feature Importance Summary\n",
    "---\n",
    "<br><br>\n",
    "As a second preparatory step, we want to perform feature selection for each of our classifiers.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-Importance-Summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:12:10.073619Z",
     "start_time": "2020-03-30T16:00:12.673121Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# estimator list for model-specific feature importance techniques\n",
    "estimators = [\n",
    "    LogisticRegression,\n",
    "    XGBClassifier,\n",
    "    RandomForestClassifier,\n",
    "    KNeighborsClassifier,\n",
    "]\n",
    "\n",
    "# instantiate FeatureSelector object\n",
    "fs = mlmachine_titanic_train.FeatureSelector(\n",
    "    data=mlmachine_titanic_train.data,\n",
    "    target=mlmachine_titanic_train.target,\n",
    "    estimators=estimators,\n",
    ")\n",
    "\n",
    "# run full feature selector suite, use accuracy metric and \n",
    "# 0 CV folds where applicable\n",
    "feature_selector_summary = fs.feature_selector_suite(\n",
    "    sequential_scoring=\"accuracy\",\n",
    "    sequential_n_folds=0,\n",
    "    add_stats=True,\n",
    "    n_jobs=1,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exhaustively Iterative Feature Selection\n",
    "---\n",
    "<br><br>\n",
    "For our final preparatory step, we use this feature selection summary to perform iterative cross-validation on smaller and smaller subsets of features for each of our estimators:\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Exhaustively-Iterative-Feature-Selection'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:13:59.285393Z",
     "start_time": "2020-03-30T16:12:10.075937Z"
    }
   },
   "outputs": [],
   "source": [
    "# use cross validation on progressively smaller feature subsets\n",
    "# to find optimal feature set\n",
    "cv_summary = fs.feature_selector_cross_val(\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "    estimators=estimators,\n",
    "    scoring=[\"accuracy\"],\n",
    "    n_folds=5,\n",
    "    step=1,\n",
    "    n_jobs=4,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "From this result, we extract our dictionary of optimum feature sets for each estimator.\n",
    "<br><br>\n",
    "The keys are estimator names, and the associated values are lists containing the column names of the best performing feature subset for each estimator. Taking  `XGBClassifier()` as an example, we used only 10 of the available 43 features to achieve the best average cross-validation accuracy on the validation dataset.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:13:59.318548Z",
     "start_time": "2020-03-30T16:13:59.287759Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feature selector summary dictionary\n",
    "cross_val_feature_dict = fs.create_cross_val_features_dict(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "With our processed dataset and optimum feature subsets in hand, it's time to use Bayesian optimization to tune the hyperparameters of our 4 estimators.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Outline Our Feature Space\n",
    "---\n",
    "<br><br>\n",
    "First, we need to establish our feature space for each parameter for each estimator:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Outline-Our-Feature-Space'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:13:59.467154Z",
     "start_time": "2020-03-30T16:13:59.320687Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from hyperopt import hp\n",
    "\n",
    "# create hyperopt parameter space for set of estimators\n",
    "estimator_parameter_space = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": hp.loguniform(\"C\", np.log(0.001), np.log(0.2)),\n",
    "        \"penalty\": hp.choice(\"penalty\", [\"l2\"]),\n",
    "    },\n",
    "    \"XGBClassifier\": {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.0, 10),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.3),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 20),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.3, 1),\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"bootstrap\": hp.choice(\"bootstrap\", [True, False]),\n",
    "        \"max_depth\": hp.choice(\"max_depth\", np.arange(2, 20, dtype=int)),\n",
    "        \"n_estimators\": hp.choice(\"n_estimators\", np.arange(100, 10000, 1, dtype=int)),\n",
    "        \"max_features\": hp.choice(\"max_features\", [\"auto\", \"sqrt\"]),\n",
    "        \"min_samples_split\": hp.choice(\n",
    "            \"min_samples_split\", np.arange(2, 40, dtype=int)\n",
    "        ),\n",
    "        \"min_samples_leaf\": hp.choice(\"min_samples_leaf\", np.arange(2, 40, dtype=int)),\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"algorithm\": hp.choice(\"algorithm\", [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]),\n",
    "        \"n_neighbors\": hp.choice(\"n_neighbors\", np.arange(1, 20, dtype=int)),\n",
    "        \"weights\": hp.choice(\"weights\", [\"distance\", \"uniform\"]),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The outermost keys of the dictionary are names of classifiers, represented by strings. The associated values are also dictionaries, where the keys are parameter names, represented as strings, and the values are hyperopt sampling distributions from which parameter values will be chosen.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run the Bayesian Optimization Job\n",
    "---\n",
    "<br><br>\n",
    "Now we're ready to run our Bayesian optimization hyperparameter tuning job. We will use a built-in method belonging to our Machine object called exec_bayes_optim_search(). Let's see mlmachine in action:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Run-the-Bayesian-Optimization-Job'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:12.245948Z",
     "start_time": "2020-03-30T16:13:59.472303Z"
    }
   },
   "outputs": [],
   "source": [
    "# execute bayesian optimization grid search\n",
    "mlmachine_titanic_train.exec_bayes_optim_search(\n",
    "    estimator_parameter_space=estimator_parameter_space,\n",
    "    data=mlmachine_titanic_train.data,\n",
    "    target=mlmachine_titanic_train.target,\n",
    "    columns=cross_val_feature_dict,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=5,\n",
    "    n_jobs=5,\n",
    "    iters=20,\n",
    "    show_progressbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Let's review the parameters:\n",
    "- `estimator_parameter_space`: The dictionary-based feature space we setup above.\n",
    "- `data`: Our observations.\n",
    "- `target`: Our target data.\n",
    "- `columns`: An optional parameter that allows us to subset the input dataset features. Accepts a list of feature names, which will apply equally to all estimators. Also accepts a dictionary, where the keys represent estimator class names and values are lists of feature names to be used with the associated estimator. In this example, we use the latter by passing in the dictionary returned by `cross_val_feature_dict` in the - `FeatureSelector` workflow above.\n",
    "- `scoring`: The scoring metric to be evaluated.\n",
    "- `n_folds`: Number of folds to use in cross-validation procedure.\n",
    "- `iters`: Total number of iterations to run the hyperparameter tuning process. In this example, we run the experiment for 200 iterations.\n",
    "- `show_progressbar`: Controls whether progress bar displays and actively updates during the course of the process.\n",
    "<br><br>\n",
    "\n",
    "Anyone familiar with hyperopt will be wondering where the objective function is. mlmachine abstracts away this complexity. \n",
    "<br><br>\n",
    "\n",
    "The process runtime depends on several attributes, including hardware, the number and type of estimators used, the number of folds, feature selection, and the number of sampling iterations. Runtimes can be quite lengthy. For this reason, `exec_bayes_optim_search()` automatically saves the result of each iteration to a CSV.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Results-Analysis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summary\n",
    "---\n",
    "<br><br>\n",
    "Let's start by loading and reviewing the results:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Results-Summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:12.277839Z",
     "start_time": "2020-03-30T16:19:12.249361Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload bayes optimization summary\n",
    "bayes_optim_summary = pd.read_csv(\"data/bayes_optimization_summary_accuracy.csv\", na_values=\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:12.299909Z",
     "start_time": "2020-03-30T16:19:12.279968Z"
    }
   },
   "outputs": [],
   "source": [
    "bayes_optim_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our Bayesian optimization log maintains key information about each iteration:\n",
    "- Iteration number, estimator and scoring metric\n",
    "- Cross-validation summary statistics\n",
    "- Iteration training time\n",
    "- Dictionary of parameters used\n",
    "\n",
    "This log provides an immense amount of data for us to analyze and evaluate the effectiveness of the Bayesian optimization process.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Optimization Assessment\n",
    "---\n",
    "<br><br>\n",
    "First and foremost, we want to see how if performance improved over the iterations.\n",
    "<br><br>\n",
    "\n",
    "Let's visualize the `XGBClassifier()` loss by iteration:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-Optimization-Assessment'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:12.789226Z",
     "start_time": "2020-03-30T16:19:12.301996Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate model loss by iteration plot for each estimator\n",
    "mlmachine_titanic_train.model_loss_plot(\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    estimator_class=\"XGBClassifier\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Each dot represents the performance of one of our 200 experiments. The key detail to notice is that the line of best fit has a clear downward slope -  exactly what we want. This means that with each iteration, model performance tends to improve compared to the previous iterations.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parameter Selection Assessment\n",
    "---\n",
    "<br><br>\n",
    "One of the coolest parts of Bayesian optimization is seeing how parameter selection is optimized.\n",
    "<br><br>\n",
    "\n",
    "For each model and for each model's parameters, we can generate a two-panel visual.\n",
    "<br><br>\n",
    "\n",
    "For numeric parameters, such as `n_estimators` or `learning_rate`, the two-visual panel includes:\n",
    "- Parameter selection KDE, overplayed on a theoretical distribution KDE\n",
    "- Parameter selection by iteration scatter plot, with line of best fit\n",
    "<br><br>\n",
    "\n",
    "For categorical parameters, such as loss function, the two-visual panel includes:\n",
    "- Parameter selection and theoretical distribution bar chart\n",
    "- Parameter selection by iteration scatter plot, faceted by parameter category\n",
    "\n",
    "Let's review the parameter selection panels for `KNeighborsClassifier()`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Parameter-Selection-Assessment'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:14.981590Z",
     "start_time": "2020-03-30T16:19:12.791513Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generate parameter selection panels for each parameter and estimator\n",
    "mlmachine_titanic_train.model_param_plot(\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    estimator_class=\"KNeighborsClassifier\",\n",
    "    estimator_parameter_space=estimator_parameter_space,\n",
    "    n_iter=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The built-in method `model_param_plot()` cycles through of the estimator's parameters and presents the appropriate panel given each parameter's type. Let's look at a numeric parameter and categorical parameter separately.\n",
    "<br><br>\n",
    "\n",
    "First, we'll review the panel for the numeric parameter `n_neighbors`:\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p5_param_panel.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "On the left, we can see two overlapping kernel density plots summarizing the actual parameter selections and the theoretical parameter distribution. The purple line corresponds to the theoretical distribution, and, as expected, this curve is smooth and evenly distributed. The teal line corresponds to the actual parameter selections, and it's clearly evident that hyperopt prefers values between 5 and 10. \n",
    "<br><br>\n",
    "\n",
    "On the right, the scatter plot visualizes the `n_neighbors` value selections over the iterations. There is a slight downward slope to the line of best fit, as the Bayesian optimization process hones in on values around 7.\n",
    "<br><br>\n",
    "\n",
    "Next, we'll review the panel for the categorical parameter `algorithm`:\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p5_param_panel_2.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "On the left, we see a bar chart displaying the counts of parameter selections, faceted by actual parameter selections and selections from the theoretical distribution . The purple bars, representing selections from the theoretical distribution, are more even than the teal bars, representing the actual selection. \n",
    "<br><br>\n",
    "\n",
    "On the right, the scatter plot again visualizes the algorithm value selection over the iterations. There is a clear decrease in selection of \"ball_tree\" and \"auto\" in favor of \"kd_tree\" and \"brute\" over the the iterations.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model-Reinstantiation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Model-Reinstantiation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Top Model Identification\n",
    "---\n",
    "<br><br>\n",
    "Our `Machine()` object has a built-in method called `top_bayes_optim_models()`, which identifies the best model for each estimator type based on the results in our Bayesian optimization log.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Top-Model-Identification'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:15.000860Z",
     "start_time": "2020-03-30T16:19:14.983600Z"
    }
   },
   "outputs": [],
   "source": [
    "# identify top performing model for each estimator\n",
    "top_models = mlmachine_titanic_train.top_bayes_optim_models(\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    num_models=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "With this method, we can identify the top N models for each estimator based on mean cross-validation score. In this experiment, `top_bayes_optim_models()` returns the dictionary below, which tells us that `LogisticRegression()` identified its top model on iteration 30, `XGBClassifier()` on iteration 61, `RandomForestClassifier()` on iteration 46, and `KNeighborsClassifier()` on iteration 109.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:15.006707Z",
     "start_time": "2020-03-30T16:19:15.002771Z"
    }
   },
   "outputs": [],
   "source": [
    "top_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Putting the Models to Use\n",
    "---\n",
    "<br><br>\n",
    "To reinstantiate a model, we leverage our `Machine()` object's built-in method `BayesOptimClassifierBuilder()`. To use this method, we pass in our results log, specify an estimator class and iteration number. This will instantiate a model object with the parameters stored on that record of the log:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Putting-the-Models-to-Use'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:15.033955Z",
     "start_time": "2020-03-30T16:19:15.009066Z"
    }
   },
   "outputs": [],
   "source": [
    "# reinstantiate top performing RandomForestClassifier\n",
    "model = mlmachine_titanic_train.BayesOptimClassifierBuilder(\n",
    "    bayes_optim_summary=bayes_optim_summary,\n",
    "    estimator_class=\"RandomForestClassifier\",\n",
    "    model_iter=46\n",
    ")\n",
    "print(model.custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The models instantiated with `BayesOptimClassifierBuilder()` use `.fit()` and `.predict()` in a way that should feel quite familiar.\n",
    "<br><br>\n",
    "\n",
    "Let's finish this article with a very basic model performance evaluation. We will fit this `RandomForestClassifier()` on the training data and labels, generate predictions with the training data, and evaluate the model's performance by comparing these predictions to the ground-truth:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T16:19:20.640853Z",
     "start_time": "2020-03-30T16:19:15.036080Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(mlmachine_titanic_train.data, mlmachine_titanic_train.target)\n",
    "\n",
    "# generate predictions\n",
    "y_pred_train = model.predict(mlmachine_titanic_train.data)\n",
    "\n",
    "# summarize results\n",
    "training_accuracy = sum(y_pred_train == mlmachine_titanic_train.target) / len(y_pred_train)\n",
    "print(\"RandomForestClassifier, iter = 46 \\nTraining accuracy: {:.2%}\".format(training_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Star the [GitHub repository](https://github.com/petersontylerd/mlmachine), and stay tuned for additional notebooks.\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
