{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mlmachine - Crowd-Sourced Feature Selection__\n",
    "<br><br>\n",
    "Welcome to Example Notebook 3. If you're new to mlmachine, check out [Example Notebook 1](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_1.ipynb) and [Example Notebook 2](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_2.ipynb).\n",
    "<br><br>\n",
    "Check out the [GitHub repository](https://github.com/petersontylerd/mlmachine).\n",
    "<br><br>\n",
    "\n",
    "1. [Crowd-Sourced Feature Importance Estimation](#Crowd-Sourced-Feature-Importance-Estimation)\n",
    "    1. [Catalog of Techniques](#Catalog-of-Techniques)\n",
    "    1. [Prepare Data](#Prepare-Data)\n",
    "    1. [FeatureSelector](#FeatureSelector)\n",
    "        1. [Example 1 - Estimator Classes](#Example-1-Estimator-Classes)\n",
    "        1. [Example 2 - Instantiated Models](#Example-2-Instantiated-Models)\n",
    "    1. [Crowd-sourcing](#Crowd-sourcing)\n",
    "1. [Feature Selection Through Iterative Cross-validation](#Feature-Selection-Through-Iterative-Cross-validation)\n",
    "    1. [CV Summary](#CV-Summary)\n",
    "    1. [Visualize Performance Curves](#Visualize-Performance-Curves)\n",
    "    1. [Results Summaries](#Results-Summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Crowd-Sourced Feature Importance Estimation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Crowd-Sourced-Feature-Importance-Estimation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Catalog of Techniques\n",
    "---\n",
    "<br><br>\n",
    "Here is a non-exhaustive list of feature importance estimation techniques:\n",
    "- Tree-based Feature Importance\n",
    "- Recursive Feature Elimination\n",
    "- Sequential Forward Selection\n",
    "- Sequential Backward Selection\n",
    "- F-value / p-value\n",
    "- Variance \n",
    "- Target Correlation\n",
    "<br><br>\n",
    "\n",
    "This battery of techniques stems from several different libraries. Ideally, we use all of these techniques (where applicable) to get a broad understanding of the role each feature plays in a machine learning problem. This is a cumbersome series of tasks.\n",
    "<br><br>\n",
    "\n",
    "Even if we took time to execute each method, disparate execution leads to disparate variables, making a holistic assessment tedious to compile.\n",
    "<br><br>\n",
    "\n",
    "mlmachine's `FeatureSelector()` class makes it easy to run all of the feature importance estimation techniques listed above. Further, we can do this for a variety of estimators simultaneously. Let's see mlmachine in action. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Catalog-of-Techniques'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare Data\n",
    "---\n",
    "<br><br>\n",
    "First, we apply data preprocessing techniques to clean up our data. We'll start by creating two `Machine()` objects - one for the training data and a second for the validation data:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Prepare-Data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:25:03.326580Z",
     "start_time": "2020-04-03T04:25:01.828340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\mlmachine-env2\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> category label encoding\n",
      "\n",
      "\t0 --> 0\n",
      "\t1 --> 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import mlmachine tools\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "\n",
    "# use titanic() function to create DataFrames for training and validation datasets\n",
    "df_train, df_valid = titanic()\n",
    "\n",
    "# ordinal encoding hierarchy\n",
    "ordinal_encodings = {\"Pclass\": [1, 2, 3]}\n",
    "\n",
    "# instantiate a Machine object for the training data\n",
    "mlmachine_titanic_train = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "# instantiate a Machine object for the validation data\n",
    "mlmachine_titanic_valid = mlm.Machine(\n",
    "    data=df_valid,\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings=ordinal_encodings,\n",
    "    is_classification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Now we process the data by imputing nulls and applying various binning and encoding techniques:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:25:05.645948Z",
     "start_time": "2020-04-03T04:25:03.327578Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, KBinsDiscretizer, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from category_encoders import WOEEncoder, TargetEncoder, CatBoostEncoder\n",
    "\n",
    "# import mlmachine tools\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    PandasFeatureUnion,\n",
    "    GroupbyImputer,\n",
    "    KFoldEncoder,\n",
    ")\n",
    "\n",
    "### create imputation PandasFeatureUnion pipeline\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"age\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\",\"SibSp\"]),\n",
    "        GroupbyImputer(null_column=\"Age\", groupby_column=\"SibSp\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"fare\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Fare\",\"Pclass\"]),\n",
    "        GroupbyImputer(null_column=\"Fare\", groupby_column=\"Pclass\", strategy=\"mean\")\n",
    "    )),\n",
    "    (\"embarked\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"Age\",\"Fare\",\"Embarked\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = impute_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = impute_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "### create simple encoding & binning PandasFeatureUnion pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=mlmachine_titanic_train.data.mlm_dtypes[\"nominal\"]),\n",
    "        PandasTransformer(OneHotEncoder(drop=\"first\")),\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=list(ordinal_encodings.keys())),\n",
    "        PandasTransformer(OrdinalEncoder(categories=list(ordinal_encodings.values()))),\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=mlmachine_titanic_train.data.mlm_dtypes[\"continuous\"]),\n",
    "        PandasTransformer(KBinsDiscretizer(encode=\"ordinal\")),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=mlmachine_titanic_train.data.mlm_dtypes[\"nominal\"] + list(ordinal_encodings.keys())),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = encode_pipe.fit_transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### create KFold encoding PandasFeatureUnion pipeline\n",
    "target_encode_pipe = PandasFeatureUnion([\n",
    "    (\"target\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]), \n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=True, random_state=0),\n",
    "            encoder=TargetEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"woe\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=WOEEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"catboost\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"category\"]),\n",
    "        KFoldEncoder(\n",
    "            target=mlmachine_titanic_train.target,\n",
    "            cv=KFold(n_splits=5, shuffle=False),\n",
    "            encoder=CatBoostEncoder,\n",
    "        ),\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"category\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = target_encode_pipe.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = target_encode_pipe.transform(mlmachine_titanic_valid.data)\n",
    "\n",
    "# update mlm_dtypes\n",
    "mlmachine_titanic_train.update_dtypes()\n",
    "mlmachine_titanic_valid.update_dtypes()\n",
    "\n",
    "### instantiate robust scaler\n",
    "scale = PandasTransformer(RobustScaler())\n",
    "\n",
    "# fit and transform training data, transform validation data\n",
    "mlmachine_titanic_train.data = scale.fit_transform(mlmachine_titanic_train.data)\n",
    "mlmachine_titanic_valid.data = scale.transform(mlmachine_titanic_valid.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:25:05.671952Z",
     "start_time": "2020-04-03T04:25:05.646948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_binned_5</th>\n",
       "      <th>Age_binned_5_catboost_encoded</th>\n",
       "      <th>Age_binned_5_target_encoded</th>\n",
       "      <th>Age_binned_5_woe_encoded</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_Q_catboost_encoded</th>\n",
       "      <th>Embarked_Q_target_encoded</th>\n",
       "      <th>Embarked_Q_woe_encoded</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>...</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass_ordinal_encoded</th>\n",
       "      <th>Pclass_ordinal_encoded_catboost_encoded</th>\n",
       "      <th>Pclass_ordinal_encoded_target_encoded</th>\n",
       "      <th>Pclass_ordinal_encoded_woe_encoded</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_male_catboost_encoded</th>\n",
       "      <th>Sex_male_target_encoded</th>\n",
       "      <th>Sex_male_woe_encoded</th>\n",
       "      <th>SibSp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622287</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>0.754887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>1.096340</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.608483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.660192</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>-0.724652</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.760707</td>\n",
       "      <td>1.370847</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.989747</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.314594</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>-0.053122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>0.275348</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.121216</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.991811</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377713</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.550145</td>\n",
       "      <td>-0.358207</td>\n",
       "      <td>-0.944738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>1.096340</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.760707</td>\n",
       "      <td>1.518257</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>1.018386</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.377713</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.550145</td>\n",
       "      <td>-0.164268</td>\n",
       "      <td>-0.944738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>-0.004054</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.100602</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.550145</td>\n",
       "      <td>-0.301361</td>\n",
       "      <td>-0.944738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.186887</td>\n",
       "      <td>3.967587</td>\n",
       "      <td>-5.176915</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.035279</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.839252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.660192</td>\n",
       "      <td>0.442971</td>\n",
       "      <td>0.553336</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>1.096340</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1.760707</td>\n",
       "      <td>1.518257</td>\n",
       "      <td>1.550231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-2.160748</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.956874</td>\n",
       "      <td>1.754877</td>\n",
       "      <td>2.064391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>1.096340</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>0.017703</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.237671</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.196885</td>\n",
       "      <td>-0.196302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>-0.724652</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089623</td>\n",
       "      <td>-0.035279</td>\n",
       "      <td>-0.167183</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.989747</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.237671</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.956874</td>\n",
       "      <td>1.193962</td>\n",
       "      <td>2.064391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.861687</td>\n",
       "      <td>-2.238169</td>\n",
       "      <td>0.940349</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.909780</td>\n",
       "      <td>0.823122</td>\n",
       "      <td>0.797100</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.996585</td>\n",
       "      <td>0.935964</td>\n",
       "      <td>0.991390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  Age_binned_5  Age_binned_5_catboost_encoded  \\\n",
       "0 -0.622287          -0.5                       0.196885   \n",
       "1  0.608483           1.0                       0.660192   \n",
       "2 -0.314594          -0.5                       0.196885   \n",
       "3  0.377713           0.5                      -0.550145   \n",
       "4  0.377713           0.5                      -0.550145   \n",
       "5  0.100602           0.5                      -0.550145   \n",
       "6  1.839252           1.0                       0.660192   \n",
       "7 -2.160748          -1.0                       1.956874   \n",
       "8 -0.237671          -0.5                       0.196885   \n",
       "9 -1.237671          -1.0                       1.956874   \n",
       "\n",
       "   Age_binned_5_target_encoded  Age_binned_5_woe_encoded  Embarked_Q  \\\n",
       "0                     0.754887                  0.000000         0.0   \n",
       "1                     0.000000                  0.553336         0.0   \n",
       "2                    -0.053122                  0.000000         0.0   \n",
       "3                    -0.358207                 -0.944738         0.0   \n",
       "4                    -0.164268                 -0.944738         0.0   \n",
       "5                    -0.301361                 -0.944738         1.0   \n",
       "6                     0.442971                  0.553336         0.0   \n",
       "7                     1.754877                  2.064391         0.0   \n",
       "8                    -0.196302                  0.000000         0.0   \n",
       "9                     1.193962                  2.064391         0.0   \n",
       "\n",
       "   Embarked_Q_catboost_encoded  Embarked_Q_target_encoded  \\\n",
       "0                     1.861687                   1.096340   \n",
       "1                     1.861687                  -0.724652   \n",
       "2                     1.861687                   0.275348   \n",
       "3                     1.861687                   1.096340   \n",
       "4                     1.861687                   0.000000   \n",
       "5                    -2.186887                   3.967587   \n",
       "6                     1.861687                   1.096340   \n",
       "7                     1.861687                   1.096340   \n",
       "8                     1.861687                  -0.724652   \n",
       "9                     1.861687                  -2.238169   \n",
       "\n",
       "   Embarked_Q_woe_encoded  Embarked_S  ...  Parch  Pclass_ordinal_encoded  \\\n",
       "0                0.940349         0.0  ...    0.0                     0.0   \n",
       "1                0.940349        -1.0  ...    0.0                    -2.0   \n",
       "2                0.940349         0.0  ...    0.0                     0.0   \n",
       "3                0.940349         0.0  ...    0.0                    -2.0   \n",
       "4                0.940349         0.0  ...    0.0                     0.0   \n",
       "5               -5.176915        -1.0  ...    0.0                     0.0   \n",
       "6                0.940349         0.0  ...    0.0                    -2.0   \n",
       "7                0.940349         0.0  ...    1.0                     0.0   \n",
       "8                0.940349         0.0  ...    2.0                     0.0   \n",
       "9                0.940349        -1.0  ...    0.0                    -1.0   \n",
       "\n",
       "   Pclass_ordinal_encoded_catboost_encoded  \\\n",
       "0                                -0.089623   \n",
       "1                                 1.760707   \n",
       "2                                -0.089623   \n",
       "3                                 1.760707   \n",
       "4                                -0.089623   \n",
       "5                                -0.089623   \n",
       "6                                 1.760707   \n",
       "7                                -0.089623   \n",
       "8                                -0.089623   \n",
       "9                                 0.909780   \n",
       "\n",
       "   Pclass_ordinal_encoded_target_encoded  Pclass_ordinal_encoded_woe_encoded  \\\n",
       "0                               0.000000                           -0.167183   \n",
       "1                               1.370847                            1.550231   \n",
       "2                              -0.121216                           -0.167183   \n",
       "3                               1.518257                            1.550231   \n",
       "4                              -0.092714                           -0.167183   \n",
       "5                              -0.035279                           -0.167183   \n",
       "6                               1.518257                            1.550231   \n",
       "7                               0.000000                           -0.167183   \n",
       "8                              -0.035279                           -0.167183   \n",
       "9                               0.823122                            0.797100   \n",
       "\n",
       "   Sex_male  Sex_male_catboost_encoded  Sex_male_target_encoded  \\\n",
       "0       0.0                   0.017268                 0.017703   \n",
       "1      -1.0                   0.996585                 0.989747   \n",
       "2      -1.0                   0.996585                 0.991811   \n",
       "3      -1.0                   0.996585                 1.018386   \n",
       "4       0.0                   0.017268                -0.004054   \n",
       "5       0.0                   0.017268                 0.000000   \n",
       "6       0.0                   0.017268                 0.017703   \n",
       "7       0.0                   0.017268                 0.017703   \n",
       "8      -1.0                   0.996585                 0.989747   \n",
       "9      -1.0                   0.996585                 0.935964   \n",
       "\n",
       "   Sex_male_woe_encoded  SibSp  \n",
       "0              0.000604    1.0  \n",
       "1              0.991390    1.0  \n",
       "2              0.991390    0.0  \n",
       "3              0.991390    1.0  \n",
       "4              0.000604    0.0  \n",
       "5              0.000604    0.0  \n",
       "6              0.000604    0.0  \n",
       "7              0.000604    3.0  \n",
       "8              0.991390    0.0  \n",
       "9              0.991390    1.0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlmachine_titanic_train.data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### FeatureSelector\n",
    "---\n",
    "<br><br>\n",
    "Our `DataFrame` has been imputed, encoded in a variety of ways, and has several new features. We're ready for `FeatureSelector()`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'FeatureSelector'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 1 - Estimator Classes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Example-1-Estimator-Classes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.979668Z",
     "start_time": "2020-04-03T04:25:05.672953Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'correlation_to_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c22ff7498d80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0msequential_scoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0msequential_n_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0msave_to_csv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m )\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\mlmachine-env2\\lib\\site-packages\\mlmachine\\features\\selection.py\u001b[0m in \u001b[0;36mfeature_selector_suite\u001b[1;34m(self, sequential_scoring, sequential_n_folds, rank, add_stats, n_jobs, save_to_csv, run_variance, run_importance, run_rfe, run_corr, run_f_score, run_sfs, run_sbs)\u001b[0m\n\u001b[0;32m    162\u001b[0m                                                             \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequential_n_folds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                                                         ) if run_sbs else None\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults_corr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selector_corr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_corr\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults_f_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selector_f_score_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrun_f_score\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\mlmachine-env2\\lib\\site-packages\\mlmachine\\features\\selection.py\u001b[0m in \u001b[0;36mfeature_selector_corr\u001b[1;34m(self, rank)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;31m# rename, sort correlation coefficient descending, and drop target row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mfeature_selector_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selector_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"correlation_to_target\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[0mfeature_selector_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selector_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorrelation_to_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m         \u001b[0mfeature_selector_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selector_summary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'correlation_to_target' is not defined"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# estimator list for model-specific feature importance techniques\n",
    "estimators = [\n",
    "    LogisticRegression,\n",
    "    XGBClassifier,\n",
    "]\n",
    "\n",
    "# instantiate FeatureSelector object\n",
    "fs = mlmachine_titanic_train.FeatureSelector(\n",
    "    data=mlmachine_titanic_train.data,\n",
    "    target=mlmachine_titanic_train.target,\n",
    "    estimators=estimators,\n",
    ")\n",
    "\n",
    "# run full feature selector suite, use accuracy metric and \n",
    "# 0 CV folds where applicable\n",
    "feature_selector_summary = fs.feature_selector_suite(\n",
    "    sequential_scoring=\"accuracy\",\n",
    "    sequential_n_folds=0,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "To instantiate a `FeatureSelector()` object, we pass the training data, labels and a list of estimators. This list of estimators can contain estimator class names, variables of instantiated models, or a combination of the two. In this example, we pass the class names for `LogisticRegression()` and `XGBClassifier()`, which leverages the default settings of the estimators.\n",
    "<br><br>\n",
    "\n",
    "Our `FeatureSelector()` object contains built-in methods for each feature importance technique described above, along with a method called `feature_selector_suite()`, which combines all techniques into a single execution. \n",
    "<br><br>\n",
    "\n",
    "To execute `feature_selector_suite()`, we pass \"accuracy\" to the parameter `sequential_scoring` and 0 to the parameter `sequential_n_folds`. These parameters influence the sequential backward/forward algorithms. We also set `save_to_csv` to True to save the resulting `DataFrame` to a CSV. Here is our result:\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.980668Z",
     "start_time": "2020-04-03T04:25:01.840Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "In the `DataFrame`, some of the columns capture the value of the metric, including F-value, P-value, variance, target correlation and Tree-based Feature Importance. The remaining columns capture the order in which a feature was selected or eliminated, depending on the underlying algorithm.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Example 2 - Instantiated Models\n",
    "---\n",
    "<br><br>\n",
    "Here is another example. This time, we instantiate `FeatureSelector()` using a list of estimators containing the `RandomForestClassifier()` class and three instantiated models. Each `RandomForestClassifier()` is instantiated with a different value for the `max_depth` hyperparameter. Lastly, we pass \"roc_auc\" to `sequential_scoring` instead of \"accuracy\" as our scoring metric.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Example-2-Instantiated-Models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.981669Z",
     "start_time": "2020-04-03T04:25:01.843Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# instantiate custom models\n",
    "rf2 = RandomForestClassifier(max_depth=2)\n",
    "rf4 = RandomForestClassifier(max_depth=4)\n",
    "rf6 = RandomForestClassifier(max_depth=6)\n",
    "\n",
    "# estimator list for default RF and three custom RFs\n",
    "estimators = [\n",
    "    RandomForestClassifier,\n",
    "    rf2,\n",
    "    rf4,\n",
    "    rf6,\n",
    "]\n",
    "\n",
    "# instantiate FeatureSelector object\n",
    "fs = mlmachine_titanic_train.FeatureSelector(\n",
    "    data=mlmachine_titanic_train.data,\n",
    "    target=mlmachine_titanic_train.target,\n",
    "    estimators=estimators,\n",
    ")\n",
    "\n",
    "# run full feature selector suite, use ROC AUC metric and \n",
    "# 0 CV folds where applicable\n",
    "feature_selector_summary = fs.feature_selector_suite(\n",
    "    sequential_scoring=\"roc_auc\",\n",
    "    sequential_n_folds=0,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.982669Z",
     "start_time": "2020-04-03T04:25:01.847Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Notice that in this `DataFrame` we have columns for Recursive Feature Elimination and Tree-based Feature Importance named specifically for the instantiated models in our estimators list.\n",
    "<br><br>\n",
    "\n",
    "We've already compiled a tidy summary describing feature importance, but we're just getting started.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Crowd-sourcing\n",
    "---\n",
    "<br><br>\n",
    "The value of having all of this information in one place is obvious, but to enable a crowd-sourced, ensemble-like assessment of feature importance we first need to normalize these values. This facilitates ranking. \n",
    "<br><br>\n",
    "\n",
    "In addition to bringing the values on to the same scale, we need to ensure that we consistently handle the directionality of the values. For example, high F-values suggest important features, whereas with p-values its the low values that suggest important features.\n",
    "<br><br>\n",
    "\n",
    "mlmachine makes this easy to do. `FeatureSelector()` contains a method called \n",
    "`feature_selector_stats()` which applies this ranking, column by column. As an added bonus, `feature_selector_stats()` adds several summary statistic columns describing each feature's ranks, and automatically sorts the features by best rank.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Crowd-sourcing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.983669Z",
     "start_time": "2020-04-03T04:25:01.851Z"
    }
   },
   "outputs": [],
   "source": [
    "# add summary statistics to feature selector summary\n",
    "feature_selector_summary = fs.feature_selector_stats(feature_selector_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.983669Z",
     "start_time": "2020-04-03T04:25:01.855Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_selector_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Notice 3 key differences in this updated `DataFrame`:\n",
    "- The values in each column in the original `DataFrame` now appear as ranks, where lower values indicate features of higher importance.\n",
    "- Additional summary statistic columns inserted on the left side of the `DataFrame`. \n",
    "- The `DataFrame` is sorted based on average rank, ascending .\n",
    "<br><br>\n",
    "\n",
    "According to this summary, the top three most important features in these models are \"Fare\", the mean-encoded version of \"Sex\", and \"Age\".\n",
    "<br><br>\n",
    "\n",
    "In this example, we executed `feature_selector_stats()` as a second step, but we can skip this by setting the `add_stats` parameter in `feature_selector_suite()` to True.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Feature Selection Through Iterative Cross-validation\n",
    "---\n",
    "<br><br>\n",
    "Feature importance alone is a good start, but our ultimate goal to select only the most meaningful features for our forthcoming model training phase. \n",
    "<br><br>\n",
    "\n",
    "Our ranked and sorted feature importance summary is the cornerstone of this next step. To evaluate each subset, `FeatureSelector()` uses an approach similar to Recursive Feature Elimination:\n",
    "- Train model with all features using cross-validation\n",
    "- Capture average performance on training and validation datasets\n",
    "- Remove least important (1 x step size) features from remaining available features \n",
    "- Repeat until feature list is depleted\n",
    "<br><br>\n",
    "\n",
    "To determine 'least important', we rely on the ranked and sorted feature importance summary. Each time we remove features, we remove the lowest ranked features from those remaining. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Feature-Selection-Through-Iterative-Cross-validation'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## CV Summary\n",
    "---\n",
    "<br><br>\n",
    "Let's see mlmachine in action:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'CV-Summary'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.984669Z",
     "start_time": "2020-04-03T04:25:01.859Z"
    }
   },
   "outputs": [],
   "source": [
    "# use cross validation on progressively smaller feature subsets\n",
    "# to find optimal feature set\n",
    "cv_summary = fs.feature_selector_cross_val(\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "    estimators=estimators,\n",
    "    scoring=\"accuracy\",\n",
    "    n_folds=5,\n",
    "    step=1,\n",
    "    n_jobs=4,\n",
    "    save_to_csv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our `FeatureSelector()` object contains a method called `feature_selector_cross_val()`. This executes the iterative feature subset evaluation and stores the result. Let's review the parameters:\n",
    "- `feature_selector_summary`: our ranked and sorted feature importance summary\n",
    "- `estimators`: the list of estimators we want to use to evaluate the features\n",
    "- `scoring`: one or more scoring metrics\n",
    "- `n_folds`: Number of folds in cross-validation procedure\n",
    "- `step`: number of features to remove after each iteration\n",
    "- `save_to_csv`: specifies whether to save the results in a CSV\n",
    "<br><br>\n",
    "\n",
    "Let's review our results:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.985669Z",
     "start_time": "2020-04-03T04:25:01.861Z"
    }
   },
   "outputs": [],
   "source": [
    "cv_summary[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "On each row of the `DataFrame`, we see the performance of a single estimator on a certain feature subset based on a specific scoring metric. The column \"features dropped\" describes how many features have been removed from the full feature set. Remember, after each iteration, `feature_selector_cross_val()` removes (1 x step) of the least important features, where importance is based on how `feature_selection_summary` is ranked and sorted.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualize Performance Curves\n",
    "---\n",
    "<br><br>\n",
    "`FeatureSelector()` also has a built-in method for visualizing the training and validation scores for each subset:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Visualize-Performance-Curves'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.985669Z",
     "start_time": "2020-04-03T04:25:01.864Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize training and validation performance for each feature subset\n",
    "fs.feature_selector_results_plot(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "    title_scale=0.8,\n",
    "    marker_on=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Here we see the training and validation accuracy scores trended for each of our 4 `RandomForestClassifier()` models. Each chart header clearly informs us of the best validation accuracy score, as well as how many features were removed from the full feature set when achieving that score.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Results Summaries\n",
    "---\n",
    "<br><br>\n",
    "Lastly, there are a couple utilities within FeatureSelector that help us summarize and utilize the results of our cross validation procedure. \n",
    "<br><br>\n",
    "\n",
    "First, the method `create_cross_val_features_df()` presents a summary of the features used by each model when achieving its best validation score:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Results-Summaries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.986669Z",
     "start_time": "2020-04-03T04:25:01.866Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feature selection summary DataFrame\n",
    "cross_val_features_df = fs.create_cross_val_features_df(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.987669Z",
     "start_time": "2020-04-03T04:25:01.869Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our features form the index of this `DataFrame`, and our estimators are represented as columns. At the intersection of each value, an X indicates if that feature was used in the subset that achieved the best validation score. The \"count\" column totals the number of estimators that used a feature, and the `DataFrame` is ranked descending on this column. \n",
    "<br><br>\n",
    "\n",
    "Second, the method `create_cross_val_features_df()` compiles the best feature subsets for each model and returns the results in a dictionary. The estimators are the keys and the associated values are list containing the best feature subset for each estimator.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.988670Z",
     "start_time": "2020-04-03T04:25:01.871Z"
    }
   },
   "outputs": [],
   "source": [
    "# create feature selection summary dictionary\n",
    "cross_val_feature_dict = fs.create_cross_val_features_dict(\n",
    "    scoring=\"accuracy_score\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "This dictionary facilitates quick utilization of these feature subsets in the model training phase.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:30:27.989670Z",
     "start_time": "2020-04-03T04:25:01.873Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_val_feature_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Star the [GitHub repository](https://github.com/petersontylerd/mlmachine), and stay tuned for additional notebooks.\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
