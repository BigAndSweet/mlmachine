{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mlmachine - Part 1: Overview, Data Intake & EDA__\n",
    "\n",
    "1. [What is mlmachine?](#What-is-mlmachine?)\n",
    "1. [The Machine Class - A Hub With Many Spokes](#The-Machine-Class-A-Hub-With-Many-Spokes)\n",
    "    1. [Pandas dtype != mlm dtype](#Pandas-dtype-!=-mlm-dtype)\n",
    "1. [Because EDA is Tedious and Takes Forever](#Because-EDA-is-Tedious-and-Takes-Forever)\n",
    "    1. [Continuous Features](#Continuous-Features)\n",
    "    1. [Categorical Features](#Categorical-Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:12:59.834235Z",
     "start_time": "2020-03-18T03:12:57.989964Z"
    }
   },
   "outputs": [],
   "source": [
    "# standard libary and settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import sklearn.datasets as datasets\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "from mlmachine.features.preprocessing import DataFrameSelector, PandasTransformer, PandasFeatureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What is mlmachine?\n",
    "---\n",
    "<br><br>\n",
    "Notebooks often serve as scratch paper for data scientists. Machine learning experiments tend to become messy, disjointed series of hard-coded blocks. Even if time is taken to write general purpose functions, those functions live isolated, uselessly locked away from new projects.\n",
    "\n",
    "mlmachine is a Python package that facilitates clean and organized notebook-based machine learning experimentation and accomplishes many key aspects of the experimentation life cycle.\n",
    "\n",
    "The central hub of mlmachine is the `Machine` class. A `Machine` object retains the dataset, target data and feature meta data, and has numerous built-in methods for quickly executing key parts of the experimentation workflow. A user can easily accomplish:\n",
    "<br><br>\n",
    "1. Data intake & `mlm_dtype` identification\n",
    "<br><br>\n",
    "2. Exploratory data analysis\n",
    "<br><br>\n",
    "\n",
    "```python\n",
    "mlmachine_demo.eda_cat_target_cat_feat(\n",
    "    feature=\"Embarked\",\n",
    "    legend_labels=[\"Died\",\"Survived\"],\n",
    ")\n",
    "```\n",
    "<br><br>\n",
    "![alt text](images/p1_eda_panel.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "3. Data preparation & feature engineering\n",
    "<br><br>\n",
    "4. Feature selection\n",
    "<br><br>\n",
    "```python\n",
    "fs.feature_selector_results_plot(\n",
    "    scoring=\"accuracy\",\n",
    "    cv_summary=cv_summary,\n",
    "    feature_selector_summary=feature_selector_summary,\n",
    "    marker_on=False,\n",
    ")\n",
    "```\n",
    "<br><br>\n",
    "![alt text](images/p1_feature_selection.jpeg \"Feature Selection\")\n",
    "<br><br>\n",
    "5. Hyperparameter tuning with Bayesian optimization\n",
    "<br><br>\n",
    "Plus, all of this occurs in a framework that keeps data in a Pandas `DataFrame`. See how the output differs simply by wrapping `PandasTransformer` around the Scikit-learn `OneHotEncoder` class in this example:\n",
    "<br><br>\n",
    "![alt text](images/p1_pandastransformer.jpeg \"PandasTransformer\")\n",
    "<br><br>\n",
    "\n",
    "To see mlmachine in action, follow the article series:\n",
    "<br><br>\n",
    "[Part 1 - Overview, Data Intake & EDA](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_1.ipynb)\n",
    "<br><br>\n",
    "[Part 2 - Pandas In / Pandas Out Pipelines](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_2.ipynb)\n",
    "<br><br>\n",
    "[Part 3 - Data Preparation & Feature Engineering](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_3.ipynb)\n",
    "<br><br>\n",
    "[Part 4 - Feature Selection](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_4.ipynb)\n",
    "<br><br>\n",
    "[Part 5 - Hyperparameter Tuning with Bayesian Optimization](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_5.ipynb)\n",
    "<br><br>\n",
    "Let's proceed with Part 1.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'What-is-mlmachine?'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Machine Class - A Hub With Many Spokes\n",
    "---\n",
    "<br><br>\n",
    "We start by instantiating a Machine object:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-Machine-Class-A-Hub-With-Many-Spokes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:14:15.496597Z",
     "start_time": "2020-03-18T03:14:15.464590Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_valid = titanic()\n",
    "\n",
    "mlmachine_titanic = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings={\"Pclass\": [1, 2, 3]},\n",
    "    is_classification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Let's unpack what we just did. Using the canonical Titanic dataset, we instantiated a `Machine` object, called `mlmachine_titanic`, by:\n",
    "\n",
    "- Passing in the full dataset as a DataFrame\n",
    "- Specifying the column that contains the target variable\n",
    "- Specifying the supervised learning task as a classification task\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The most basic purpose of `mlmachine_titanic` is to maintain our dataset of observations and our target values. Our dataset is stored as a `DataFrame` and can be accessed by calling `mlmachine_titanic.data`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:14:44.221267Z",
     "start_time": "2020-03-18T03:14:44.200266Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our target variable, stored as a named Pandas `Series`, can be accessed just as easily by calling `mlmachine_titanic.target`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:15:08.923978Z",
     "start_time": "2020-03-18T03:15:08.919976Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "We also passed several lists containing feature names to parameters such as `identify_as_continuous` and `identify_as_nominal`, along with directions on how to encode an ordinal column, and which features to remove. Let's get into the purpose of these parameters.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pandas dtype != mlm dtype\n",
    "---\n",
    "<br><br>\n",
    "Pandas dtypes describe the values contained within a column, but have no regard for what the values actually mean. Nominal categories, ordinal categories, continuous numbers, counts…it's often impossible to make these distinctions from Pandas dtypes alone. In the world of mlmachine, these distinctions are referred to as `mlm_dtypes`. mlmachine catalogs and, most importantly, updates `mlm_dtypes` as the dataset evolves through feature engineering.\n",
    "<br><br>\n",
    "Why does this even matter? Because `mlm_dtypes` inform several key questions throughout the experimentation workflow, namely:\n",
    "- EDA - which visual types are most effective?\n",
    "- Imputing - which imputation techniques are most reasonable?\n",
    "- Encoding - which encoding techniques should be used, if any?\n",
    "- Feature engineering - which techniques, such as polynomial feature creation, binning and target encoding, should be used, and for which features?\n",
    "<br><br>\n",
    "\n",
    "Answers to these questions stem directly from the nature of the feature values. Further, we often want to act upon several features of a certain type at once, which would typically require managing hard-coded lists of feature names. This is a pain.\n",
    "<br><br>\n",
    "\n",
    "With that motivation in mind, let's revisit our `Machine` object, mlmachine_titanic. Our `DataFrame`, which we store as `mlmachine_titanic.data`, has a metadata attribute called mlm_dtypes:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pandas-dtype-!=-mlm-dtype'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:15:08.923978Z",
     "start_time": "2020-03-18T03:15:08.919976Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.data.mlm_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Notice the keys of the dictionary. Per the guidance we provided when instantiating the `Machine` object, `mlmachine_titanic.data.mlm_dtypes` is remembering the mlm dtype for each feature. Further, notice we had the option to specify \"boolean\", \"string\" and \"date\" columns if needed. Lastly, the key \"category\" is a simple concatenation of the keys \"nominal\", \"ordinal\" and \"boolean\" values, and the key \"number\" is a concatenation of the keys \"continuous\" and \"count\" values. These are merely high-level key/value pairs provided for convenience.\n",
    "<br><br>\n",
    "\n",
    "We can easily update `mlm_dtypes` to reflect the current state of the dataset. By calling the method `update_mlm_dtypes()`, mlmachine evaluates the features currently in the data and updates `mlm_dtypes` accordingly.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:15:45.098415Z",
     "start_time": "2020-03-18T03:15:45.055404Z"
    }
   },
   "outputs": [],
   "source": [
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal_columns\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"nominal\"]),\n",
    "        PandasTransformer(OneHotEncoder()),\n",
    "    )),\n",
    "    (\"other_columns\", make_pipeline(\n",
    "        DataFrameSelector(exclude_mlm_dtypes=[\"nominal\"]),\n",
    "    )),\n",
    "])\n",
    "\n",
    "mlmachine_titanic.data = encode_pipe.fit_transform(mlmachine_titanic.data.dropna(axis=0))\n",
    "mlmachine_titanic.update_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "A lot just happened here, and all will become clear in Part 2. For now, focus on the fact that we just one hot encoded our nominal category columns (line 4) and updated our `mlm_dtypes` dictionary. The result of executing `mlmachine_titanic.update_mlm_dtypes()` on line 12 is below. Notice that the value of our \"nominal\" key has been updated. The original features \"Embarked\" and \"Sex\" no longer exist in the dataset, so they no longer exist in `mlm_dtypes`. Instead, these two features have been replaced with their respective dummy columns that were created by `OneHotEncoder`.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:15:49.999140Z",
     "start_time": "2020-03-18T03:15:49.993137Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.data.mlm_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "mlmachine maintains and modifies these feature type distinctions throughout the life cycle of the experiment. The only requirement is to identify the mlm dtype for each feature from the outset (something we should do every time anyway).\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Because EDA is Tedious and Takes Forever\n",
    "---\n",
    "<br><br>\n",
    "We're all guilty of performing a cursory EDA, if any at all (\"let's just get to the model training!\"). Even with all of the great Python visualization libraries out there, EDA can take a considerable amount of setup. Coding those same, slightly modified functions for the hundredth time is something we all do. And remembering which visual types work best for which feature types and feature/target type combination is not easy.\n",
    "<br><br>\n",
    "\n",
    "Skipping EDA is absolutely a mistake, so a portion of mlmachine's functionality is dedicated to quickly making panels that are as beneficial as they are good looking.\n",
    "<br><br>\n",
    "\n",
    "We saw one example in the Overview section of this article. Let's look at another:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Because-EDA-is-Tedious-and-Takes-Forever'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Continuous Features\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-Features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:16:10.399216Z",
     "start_time": "2020-03-18T03:16:09.711105Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.eda_cat_target_num_feat(\n",
    "    feature=\"Age\",\n",
    "    legend_labels=[\"Died\",\"Survived\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "`eda_cat_target_num_feat` is a method associated with `mlmachine_titanic` that generates a panel for evaluating a numeric feature in the context of a categorical target. At the top, we display three Pandas DataFrames:\n",
    "1. Feature Summary - All of the summary statistics we get by executing the standard `df.describe()` command, plus \"percent missing\", \"skew\" and \"kurtosis\".\n",
    "2. Feature vs. target summary -  Count, proportion, mean and standard deviation of the numeric feature, grouped by the different classes in the target.\n",
    "3. Statistical test - If the target column only has two classes, this reports the result of a z-test (or t-test, in the case of small samples) and the associated p-value. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Below the summary tables is a panel containing four visualizations. From left to right, starting in the top left corner:\n",
    "1. Univariate distribution plot of the numeric feature.\n",
    "2. QQ plot of the numeric feature.\n",
    "3. Bivariate distribution plot of the numeric feature, faceted by the target.\n",
    "4. Horizontal box plot, faceted by the target.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "This visualization function adapts to multi-class problems easily. Let's look at another quick example:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:16:51.011304Z",
     "start_time": "2020-03-18T03:16:50.296190Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_wine()\n",
    "dataset = pd.merge(\n",
    "            pd.DataFrame(dataset.data, columns=dataset.feature_names),\n",
    "            pd.Series(dataset.target, name=\"Class label\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "mlmachine_wine = mlm.Machine(\n",
    "    data=dataset,\n",
    "    identify_as_continuous=list(dataset.columns[:-1]),\n",
    "    target=\"Class label\",\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "mlmachine_wine.eda_cat_target_num_feat(\n",
    "    feature=\"alcalinity_of_ash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "We created this using the Scikit-learn wine dataset with the same minimal code. Notice the changes:\n",
    "1. The \"Feature vs. target summary\" table expands to reflect all three classes.\n",
    "2. The faceted plots expand to visualize all three classes.\n",
    "3. The x-axis and y-axis tick labels are decimals rather than whole numbers. This modification happens dynamically under the hood based on the scale of the data being visualized. Less time formatting, more time exploring.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Categorical Features\n",
    "---\n",
    "<br><br>\n",
    "Before finishing Part 1 of this series, let's revisit the teaser EDA panel from the beginning. Instead of looking at the nominal category feature \"Embarked\" again, let's look at \"SibSp\" instead, which is a count feature:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-Features'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:17:16.288156Z",
     "start_time": "2020-03-18T03:17:15.807082Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.eda_cat_target_cat_feat(\n",
    "    feature=\"SibSp\",\n",
    "    legend_labels=[\"Died\",\"Survived\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "`eda_cat_target_cat_feat` generates a panel for a categorical or count feature in the context of a categorical target. Again, there are three summary tables at the top:\n",
    "1. Feature Summary - Simple count of each level in the category, along with the percent of values each level constitutes in the feature.\n",
    "2. Feature vs. target summary - Count of each level in the category, grouped by the classes in the target\n",
    "3. Target proportion - The percent of values of a particular feature level, grouped by the classes in the target.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The panel includes three visualization. From left to right:\n",
    "1. Tree map of the categorical feature.\n",
    "2. Bar chart of the categorical feature, faceted by the target.\n",
    "3. 100% horizontal stacked bar chart, faceted by the target.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Let's close out by taking one last look at the Scikit-learn wine dataset, this time using `eda_cat_target_cat_feat` to visualize a numeric feature that has been segmented into 5 bins, effectively making it a categorical column:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T03:17:40.429867Z",
     "start_time": "2020-03-18T03:17:40.048807Z"
    }
   },
   "outputs": [],
   "source": [
    "binner = KBinsDiscretizer(n_bins=5, encode=\"ordinal\")\n",
    "\n",
    "mlmachine_wine.data[\"alcalinity_of_ash\"] = binner.fit_transform(\n",
    "    mlmachine_wine.data[\"alcalinity_of_ash\"].values.reshape(-1, 1)\n",
    ")\n",
    "\n",
    "mlmachine_wine.eda_cat_target_cat_feat(\n",
    "    feature=\"alcalinity_of_ash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Each component of the panel adapts accordingly to the multi-class problem in this dataset.\n",
    "<br><br>\n",
    "Continue to [Part 2](https://github.com/petersontylerd/mlmachine/blob/master/notebooks/mlmachine_part_2.ipynb)\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
